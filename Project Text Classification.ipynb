{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pylab import rcParams\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "X = [] # X is represented as (document's name,text of document)\n",
    "Y = [] # Y is represented as the newsgroup category of the corresponding X data\n",
    "\n",
    "for category in os.listdir('20_newsgroups'):\n",
    "    print(category)\n",
    "    for document in os.listdir('20_newsgroups/'+category):\n",
    "        with open('20_newsgroups/'+category+'/'+document, \"r\") as f:\n",
    "            X.append((document,f.read()))\n",
    "            Y.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Data in Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the Features Set on the basis of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set(x_train):\n",
    "    \n",
    "    #Getting list of stop words\n",
    "    stop_words = get_stop_words('english')\n",
    "    d = {}\n",
    "    \n",
    "    #Iterating through all the documents in the trainng set \n",
    "    for i in range(len(x_train)):\n",
    "        #Spliting the documents into words list\n",
    "        words = re.split(r'\\W+', X[i][1])\n",
    "        #Changeing all the words to lower case to maintain uniformity\n",
    "        words = [word.lower() for word in words]\n",
    "        for word in words:\n",
    "            #Checking if the words is a stop words or a number\n",
    "            if (word in stop_words) or (not word.isalpha()):\n",
    "                continue\n",
    "            d[word] = d.get(word, 0) + 1\n",
    "\n",
    "    keys = np.array(list(d.keys()))\n",
    "    values = np.array(list(d.values()))\n",
    "    \n",
    "    # Getting the top 1000 words with the highest frequency\n",
    "    ind = values.argsort()[::-1]\n",
    "    ind = ind[:2000]\n",
    "    values = values[ind]\n",
    "    keys = keys[ind]\n",
    "\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to split  a Document into Words List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(text):\n",
    "    words = re.split(r'\\W+', text)\n",
    "    words = [word.lower() for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get Data in the form of 2D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2D_data(x, feature_set):\n",
    "    \n",
    "    data = []\n",
    "    n = len(feature_set)\n",
    "    \n",
    "    #Iterating through all the documents in x\n",
    "    for i in range(len(x)):\n",
    "        #Spliting the documents into words list\n",
    "        words = get_word_list(x[i][1])\n",
    "        l = np.zeros(n, dtype = int)\n",
    "        for word in words:\n",
    "            #Checking if the word is present in the feature set or not\n",
    "            ind = np.where(feature_set == word)\n",
    "            l[ind] = l[ind] + 1\n",
    "        data.append(l)\n",
    "    return data              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Test Classification using Multinomial Naive Bayes (already implemented in sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = get_feature_set(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2d = get_2D_data(x_train, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_2d, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2d = get_2D_data(x_test, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Report\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.82      0.74      0.77       253\n",
      "           comp.graphics       0.77      0.72      0.74       249\n",
      " comp.os.ms-windows.misc       0.83      0.06      0.12       233\n",
      "comp.sys.ibm.pc.hardware       0.57      0.87      0.69       263\n",
      "   comp.sys.mac.hardware       0.77      0.90      0.83       252\n",
      "          comp.windows.x       0.81      0.83      0.82       246\n",
      "            misc.forsale       0.72      0.96      0.82       244\n",
      "               rec.autos       0.85      0.88      0.87       244\n",
      "         rec.motorcycles       0.86      0.95      0.90       238\n",
      "      rec.sport.baseball       0.95      0.91      0.93       256\n",
      "        rec.sport.hockey       0.91      0.92      0.92       247\n",
      "               sci.crypt       0.98      0.96      0.97       260\n",
      "         sci.electronics       0.86      0.86      0.86       255\n",
      "                 sci.med       0.95      0.88      0.91       255\n",
      "               sci.space       0.95      0.95      0.95       268\n",
      "  soc.religion.christian       0.96      1.00      0.98       263\n",
      "      talk.politics.guns       0.77      0.87      0.82       242\n",
      "   talk.politics.mideast       0.93      0.80      0.86       233\n",
      "      talk.politics.misc       0.73      0.64      0.68       240\n",
      "      talk.religion.misc       0.65      0.64      0.65       259\n",
      "\n",
      "                accuracy                           0.82      5000\n",
      "               macro avg       0.83      0.82      0.80      5000\n",
      "            weighted avg       0.83      0.82      0.81      5000\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "[[186   0   0   0   0   0   0   3   2   1   1   1   0   1   0   0   1   1\n",
      "    1  55]\n",
      " [  0 179   0  20  15   6  12   5   2   1   0   0   8   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0  27  15 116  11  41  17   1   0   0   0   1   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   2   0 229  22   1   5   0   0   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   1  13 226   0   6   0   0   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  13   0  13   6 204   6   1   1   0   0   0   0   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   3   0   0 234   2   2   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   9 215  13   0   4   0   2   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   5   5 225   0   0   0   1   0   1   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   3   2   3 234  13   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   3   3   1   9 228   0   0   0   1   0   0   0\n",
      "    1   0]\n",
      " [  0   2   1   0   1   0   1   0   0   0   0 250   2   0   0   0   2   0\n",
      "    1   0]\n",
      " [  0   1   0   9   7   0   7   3   0   0   2   1 220   1   4   0   0   0\n",
      "    0   0]\n",
      " [  0   4   0   1   2   0   6   3   4   0   1   0   5 225   1   0   2   0\n",
      "    1   0]\n",
      " [  0   1   0   0   2   0   0   1   0   1   0   0   3   2 254   0   2   1\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 262   0   1\n",
      "    0   0]\n",
      " [  0   0   1   0   1   0   1   3   5   0   0   2   1   1   0   0 211   2\n",
      "   10   4]\n",
      " [  2   0   0   0   2   0   8   2   3   0   1   0   2   2   0   1  11 187\n",
      "   11   1]\n",
      " [  1   0   0   0   0   0   2   2   0   1   0   0   1   1   6   2  34   8\n",
      "  153  29]\n",
      " [ 38   0   0   0   0   0   1   1   0   0   0   0   0   2   0   9   9   2\n",
      "   31 166]]\n",
      "---------------------------------------------------------------------------\n",
      "Mean Accuracy:  0.8206\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test_2d)\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Mean Accuracy: \", clf.score(x_test_2d, y_test, sample_weight=None))\n",
    "print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Naive Bayes on your own from scratch for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: As mentioned in the hint vedio I have ignored the probablities of those words in the testing data (while predicting the output) which are not present in our vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train):\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    result = {}\n",
    "    class_values = set(y_train)\n",
    "    \n",
    "    #Iterating over all possible class values\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        #Getting total number of data points\n",
    "        result[\"total_data\"] = len(y_train)\n",
    "        \n",
    "        #Getting the data for the current class\n",
    "        current_class_rows = (y_train == current_class)\n",
    "        x_train_current = x_train[current_class_rows]\n",
    "        y_train_current = y_train[current_class_rows]\n",
    "        \n",
    "        #Getting total number of data points in current class\n",
    "        result[current_class][\"total_count\"] = len(y_train_current)\n",
    "        \n",
    "        #Finding the Number of Features(Words in Vocabulary)\n",
    "        n = x_train.shape[1]\n",
    "        \n",
    "        #Storing the total Frequency of Each Word in the Current class\n",
    "        for i in range(n):\n",
    "            result[current_class][i] = x_train_current[:,i].sum()\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(dictionary, x, current_class):\n",
    "    \n",
    "    #Calculating the Prior Probabilty \n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    \n",
    "    #Finding the number of Features (Words in Vocabulary)\n",
    "    num_of_features = len(dictionary[current_class].keys()) - 1\n",
    "    \n",
    "    #Iterating over the feature set (Words in Vocabulary) to calculate the probability of Each Feature (Word)\n",
    "    for i in range(num_of_features):\n",
    "        \n",
    "        #Checking if the ith word in vocabulary is present in x or not\n",
    "        if x[i] > 0:\n",
    "            \n",
    "            #Finding the count of feature (word) in the current class\n",
    "            # Adding 1 for Laplace correction\n",
    "            current_class_ith_word_count = dictionary[current_class][i] + 1\n",
    "        \n",
    "            #Getting the total count for current class\n",
    "            count_current_class = dictionary[current_class][\"total_count\"] + len(dictionary[current_class].keys())\n",
    "            \n",
    "            #Calculating the probability of ith Feature (Word)\n",
    "            current_xi_probablity = np.log(current_class_ith_word_count) - np.log(count_current_class)\n",
    "            output = output + current_xi_probablity\n",
    "            \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_one_point(dictionary, x):\n",
    "    \n",
    "    #Getting all possible class values\n",
    "    classes_list = dictionary.keys()\n",
    "    \n",
    "    best_probality = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    \n",
    "    #Iterating over all possible class values\n",
    "    for current_class in classes_list:\n",
    "        \n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        \n",
    "        #Getting the probabilty of x belonging to the current class\n",
    "        prob_current_class = get_probability(dictionary, x, current_class)\n",
    "        \n",
    "        #Finding the best class for x\n",
    "        if (first_run or prob_current_class > best_probality):\n",
    "            best_probality = prob_current_class\n",
    "            best_class = current_class\n",
    "            \n",
    "        first_run = False\n",
    "        \n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, x_test):\n",
    "    \n",
    "    y_pred = []\n",
    "    #Iterating over each data point in the testing dataset\n",
    "    for x in x_test:\n",
    "        x_class = predict_for_one_point(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(x_train_2d, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_self = predict(dictionary, x_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Report\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.75      0.77       253\n",
      "           comp.graphics       0.46      0.88      0.60       249\n",
      " comp.os.ms-windows.misc       0.76      0.70      0.73       233\n",
      "comp.sys.ibm.pc.hardware       0.84      0.56      0.67       263\n",
      "   comp.sys.mac.hardware       0.99      0.39      0.56       252\n",
      "          comp.windows.x       0.70      0.95      0.81       246\n",
      "            misc.forsale       0.93      0.59      0.73       244\n",
      "               rec.autos       0.94      0.66      0.78       244\n",
      "         rec.motorcycles       0.99      0.76      0.86       238\n",
      "      rec.sport.baseball       0.99      0.73      0.84       256\n",
      "        rec.sport.hockey       0.89      0.95      0.92       247\n",
      "               sci.crypt       0.74      0.96      0.83       260\n",
      "         sci.electronics       0.97      0.44      0.61       255\n",
      "                 sci.med       1.00      0.54      0.70       255\n",
      "               sci.space       0.96      0.79      0.87       268\n",
      "  soc.religion.christian       1.00      0.84      0.91       263\n",
      "      talk.politics.guns       0.69      0.27      0.39       242\n",
      "   talk.politics.mideast       0.59      0.88      0.71       233\n",
      "      talk.politics.misc       0.26      0.94      0.41       240\n",
      "      talk.religion.misc       0.69      0.40      0.51       259\n",
      "\n",
      "                accuracy                           0.70      5000\n",
      "               macro avg       0.81      0.70      0.71      5000\n",
      "            weighted avg       0.81      0.70      0.71      5000\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "[[189   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   6\n",
      "   15  41]\n",
      " [  0 219   5   0   0  18   1   0   0   0   0   0   0   0   0   0   0   1\n",
      "    5   0]\n",
      " [  0  18 164   1   0  43   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    6   0]\n",
      " [  0  66  18 147   1  20   1   0   0   0   0   5   0   0   0   0   0   3\n",
      "    2   0]\n",
      " [  0  87  10  15  99  13   3   0   0   0   0  15   0   0   0   0   0   1\n",
      "    9   0]\n",
      " [  0   8   3   0   0 234   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  24  10  11   0   1 145   5   0   0   0   5   3   0   0   0   1   5\n",
      "   34   0]\n",
      " [  0   1   0   0   0   0   4 162   0   0   2   2   0   0   1   0  13  15\n",
      "   44   0]\n",
      " [  0   3   1   0   0   1   1   4 180   0   0   1   0   0   1   0   6  13\n",
      "   27   0]\n",
      " [  0   0   1   0   0   0   0   0   1 187  27   0   0   0   1   0   3   8\n",
      "   28   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2 235   0   0   0   0   0   1   1\n",
      "    8   0]\n",
      " [  0   1   1   0   0   1   0   0   0   0   0 250   0   0   0   0   0   0\n",
      "    7   0]\n",
      " [  0  40   2   2   0   2   1   1   0   0   0  45 112   0   5   0   3   7\n",
      "   34   1]\n",
      " [  2   9   1   0   0   1   0   0   0   0   0   6   0 138   1   0   0   8\n",
      "   88   1]\n",
      " [  0   3   0   0   0   0   0   0   0   0   0   3   0   0 212   0   1   5\n",
      "   44   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 221   0  15\n",
      "   27   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   3   0   0   0   0  66  35\n",
      "  135   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 206\n",
      "   25   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14\n",
      "  225   1]\n",
      " [ 44   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   6\n",
      "  102 104]]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test,y_pred_self))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test,y_pred_self))\n",
    "print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Results of your implementation of Naive Bayes with one in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Report of Naive Bayes already implemented in sklearn\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.82      0.74      0.77       253\n",
      "           comp.graphics       0.77      0.72      0.74       249\n",
      " comp.os.ms-windows.misc       0.83      0.06      0.12       233\n",
      "comp.sys.ibm.pc.hardware       0.57      0.87      0.69       263\n",
      "   comp.sys.mac.hardware       0.77      0.90      0.83       252\n",
      "          comp.windows.x       0.81      0.83      0.82       246\n",
      "            misc.forsale       0.72      0.96      0.82       244\n",
      "               rec.autos       0.85      0.88      0.87       244\n",
      "         rec.motorcycles       0.86      0.95      0.90       238\n",
      "      rec.sport.baseball       0.95      0.91      0.93       256\n",
      "        rec.sport.hockey       0.91      0.92      0.92       247\n",
      "               sci.crypt       0.98      0.96      0.97       260\n",
      "         sci.electronics       0.86      0.86      0.86       255\n",
      "                 sci.med       0.95      0.88      0.91       255\n",
      "               sci.space       0.95      0.95      0.95       268\n",
      "  soc.religion.christian       0.96      1.00      0.98       263\n",
      "      talk.politics.guns       0.77      0.87      0.82       242\n",
      "   talk.politics.mideast       0.93      0.80      0.86       233\n",
      "      talk.politics.misc       0.73      0.64      0.68       240\n",
      "      talk.religion.misc       0.65      0.64      0.65       259\n",
      "\n",
      "                accuracy                           0.82      5000\n",
      "               macro avg       0.83      0.82      0.80      5000\n",
      "            weighted avg       0.83      0.82      0.81      5000\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Classification Report of self Implemented Naive Bayes\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.75      0.77       253\n",
      "           comp.graphics       0.46      0.88      0.60       249\n",
      " comp.os.ms-windows.misc       0.76      0.70      0.73       233\n",
      "comp.sys.ibm.pc.hardware       0.84      0.56      0.67       263\n",
      "   comp.sys.mac.hardware       0.99      0.39      0.56       252\n",
      "          comp.windows.x       0.70      0.95      0.81       246\n",
      "            misc.forsale       0.93      0.59      0.73       244\n",
      "               rec.autos       0.94      0.66      0.78       244\n",
      "         rec.motorcycles       0.99      0.76      0.86       238\n",
      "      rec.sport.baseball       0.99      0.73      0.84       256\n",
      "        rec.sport.hockey       0.89      0.95      0.92       247\n",
      "               sci.crypt       0.74      0.96      0.83       260\n",
      "         sci.electronics       0.97      0.44      0.61       255\n",
      "                 sci.med       1.00      0.54      0.70       255\n",
      "               sci.space       0.96      0.79      0.87       268\n",
      "  soc.religion.christian       1.00      0.84      0.91       263\n",
      "      talk.politics.guns       0.69      0.27      0.39       242\n",
      "   talk.politics.mideast       0.59      0.88      0.71       233\n",
      "      talk.politics.misc       0.26      0.94      0.41       240\n",
      "      talk.religion.misc       0.69      0.40      0.51       259\n",
      "\n",
      "                accuracy                           0.70      5000\n",
      "               macro avg       0.81      0.70      0.71      5000\n",
      "            weighted avg       0.81      0.70      0.71      5000\n",
      "\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Classification Report of Naive Bayes already implemented in sklearn\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Classification Report of self Implemented Naive Bayes\")\n",
    "print(classification_report(y_test,y_pred_self))\n",
    "print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It can easily be observed that the overall performace of both self implemented Naive Bayes and Sklearn implemented Naive Bayes is more or less the same with one performing better for some classes (output) while other performing better for rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix of Naive Bayes already implemented in sklearn\n",
      "[[186   0   0   0   0   0   0   3   2   1   1   1   0   1   0   0   1   1\n",
      "    1  55]\n",
      " [  0 179   0  20  15   6  12   5   2   1   0   0   8   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0  27  15 116  11  41  17   1   0   0   0   1   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   2   0 229  22   1   5   0   0   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   1  13 226   0   6   0   0   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  13   0  13   6 204   6   1   1   0   0   0   0   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   3   0   0 234   2   2   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   9 215  13   0   4   0   2   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   5   5 225   0   0   0   1   0   1   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   3   2   3 234  13   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   3   3   1   9 228   0   0   0   1   0   0   0\n",
      "    1   0]\n",
      " [  0   2   1   0   1   0   1   0   0   0   0 250   2   0   0   0   2   0\n",
      "    1   0]\n",
      " [  0   1   0   9   7   0   7   3   0   0   2   1 220   1   4   0   0   0\n",
      "    0   0]\n",
      " [  0   4   0   1   2   0   6   3   4   0   1   0   5 225   1   0   2   0\n",
      "    1   0]\n",
      " [  0   1   0   0   2   0   0   1   0   1   0   0   3   2 254   0   2   1\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 262   0   1\n",
      "    0   0]\n",
      " [  0   0   1   0   1   0   1   3   5   0   0   2   1   1   0   0 211   2\n",
      "   10   4]\n",
      " [  2   0   0   0   2   0   8   2   3   0   1   0   2   2   0   1  11 187\n",
      "   11   1]\n",
      " [  1   0   0   0   0   0   2   2   0   1   0   0   1   1   6   2  34   8\n",
      "  153  29]\n",
      " [ 38   0   0   0   0   0   1   1   0   0   0   0   0   2   0   9   9   2\n",
      "   31 166]]\n",
      "---------------------------------------------------------------------------\n",
      "Confusion Matrix of self Implemented Naive Bayes\n",
      "[[189   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   6\n",
      "   15  41]\n",
      " [  0 219   5   0   0  18   1   0   0   0   0   0   0   0   0   0   0   1\n",
      "    5   0]\n",
      " [  0  18 164   1   0  43   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    6   0]\n",
      " [  0  66  18 147   1  20   1   0   0   0   0   5   0   0   0   0   0   3\n",
      "    2   0]\n",
      " [  0  87  10  15  99  13   3   0   0   0   0  15   0   0   0   0   0   1\n",
      "    9   0]\n",
      " [  0   8   3   0   0 234   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  24  10  11   0   1 145   5   0   0   0   5   3   0   0   0   1   5\n",
      "   34   0]\n",
      " [  0   1   0   0   0   0   4 162   0   0   2   2   0   0   1   0  13  15\n",
      "   44   0]\n",
      " [  0   3   1   0   0   1   1   4 180   0   0   1   0   0   1   0   6  13\n",
      "   27   0]\n",
      " [  0   0   1   0   0   0   0   0   1 187  27   0   0   0   1   0   3   8\n",
      "   28   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2 235   0   0   0   0   0   1   1\n",
      "    8   0]\n",
      " [  0   1   1   0   0   1   0   0   0   0   0 250   0   0   0   0   0   0\n",
      "    7   0]\n",
      " [  0  40   2   2   0   2   1   1   0   0   0  45 112   0   5   0   3   7\n",
      "   34   1]\n",
      " [  2   9   1   0   0   1   0   0   0   0   0   6   0 138   1   0   0   8\n",
      "   88   1]\n",
      " [  0   3   0   0   0   0   0   0   0   0   0   3   0   0 212   0   1   5\n",
      "   44   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 221   0  15\n",
      "   27   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   3   0   0   0   0  66  35\n",
      "  135   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 206\n",
      "   25   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14\n",
      "  225   1]\n",
      " [ 44   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   6\n",
      "  102 104]]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix of Naive Bayes already implemented in sklearn\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix of self Implemented Naive Bayes\")\n",
    "print(confusion_matrix(y_test,y_pred_self))\n",
    "print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
